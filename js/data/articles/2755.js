window._articleFullText = window._articleFullText || {};
window._articleFullText[2755] = "<div class=\"col-lg-12 col-md-12 col-sm-12 col-xs-12\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t<div  style=\"width:100%; float:left;  padding-bottom:0px;\">\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t<div\tstyle=\"width:100%; float:left; min-height:45px; text-align:justify\">\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t\t<a href=\"\" name=\"introduction\"></a>\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t\t<!--<h2>Introduction</h2>-->\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t\t<p>In today&rsquo;s data‑rich biomedical landscape, traditional statistical practices increasingly intersect with powerful artificial intelligence (AI) tools. This convergence presents both opportunities and challenges. AI and machine learning (ML) can reveal patterns that manual analysis might overlook, potentially accelerating discovery. However, their use also heightens longstanding concerns about validity and reproducibility. The inappropriate application of AI is leading to findings that are often unreproducible or clinically irrelevant. This editorial examines issues of reproducibility, the limitations of <em>p </em>‑value‑based inference, and the integration of AI and provides practical guidance for clinicians and researchers.</p>\r\n<p>Reproducibility has long been a concern in medical science, with flawed statistical practices contributing to the problem. In the AI era, new challenges have emerged. Modern AI models-such as deep neural networks-are&nbsp;inherently nondeterministic&nbsp;and sensitive to variations in data and hardware.<sup>1</sup> Simply sharing code does not guarantee identical results, as random initialization, complex processing pipelines, and computational differences can alter outputs. For example, Ball<sup>2 </sup>describes striking cases in which AI diagnosed disease based on image background artifacts rather than true pathology and highlights how issues such as data leakage (e.g., accidentally using test data during training) and poorly documented model changes have led to reproducibility failures across multiple fields.</p>\r\n<p>To address these issues, the research community is increasingly emphasizing transparency and standards. AI-driven studies should thoroughly report data preprocessing steps, model architectures, and training conditions. Whenever feasible, sharing data and code facilitates independent verification. Journals have begun requiring adherence to reporting guidelines-such as CONSORT-AI, TRIPOD-AI, and CLAIM-which mandate clear documentation of methods and validation procedures. Moreover, replication-ideally by independent teams or using new datasets-is essential. Replication is the most reliable way to confirm findings, as statistical inference alone, including small <em>p </em>-values, can be misleading.<sup>3</sup> In practice, clinicians should interpret AI results from single studies with caution and seek confirmatory evidence or prospective validation before applying findings in clinical care.</p>\r\n<p>Traditional hypothesis testing and <em>p </em>-values remain widely used, but their limitations are now better understood. Modern analyses often involve large datasets (&ldquo;big data&rdquo;), where even trivial differences can become&nbsp;statistically&nbsp;significant. Experts increasingly advocate moving beyond rigid thresholds. For instance, a recent review emphasizes that the <em>p </em>-value is a useful decision-making tool only when interpreted in context-taking into account study design, effect sizes, and prior knowledge.<sup>4</sup> In short, a <em>p-value</em> is just one piece of evidence, not a definitive marker of truth. The American Statistical Association&rsquo;s 2016 statement cautioned against overreliance on the &ldquo;<em>p </em>&lt; 0.05&rdquo; threshold, discouraged the use of hard cutoffs, and recommended transparent, contextual reporting of effect sizes and confidence intervals rather than focusing on a single <em>p </em>-value.<sup>5</sup></p>\r\n<p>In practice, clinicians and researchers should prioritize&nbsp;clinical significance. For example, a drug that reduces systolic blood pressure by just 1 mmHg may yield a <em>p </em>-value of 0.049 in a large trial-yet the effect is clinically negligible. Conversely, a 10 mmHg reduction with a <em>p </em>-value of 0.051 could represent a meaningful benefit that warrants further investigation, rather than dismissal based on an arbitrary threshold. Some experts have proposed replacing binary significance labels with graded measures of evidence strength-such as likelihood ratios or Bayesian factors-to better reflect how data influence our belief in a hypothesis.<sup>6,7</sup> At a minimum, <em>p </em>‑values should always be reported alongside effect sizes and confidence intervals (or Bayesian posterior estimates). Transparency demands making uncertainty explicit and avoiding overstatements driven by arbitrary statistical cut-offs.</p>\r\n<p>AI and ML are increasingly integrated into research workflows. For example, algorithms now analyze imaging data<sup>8,9 </sup>or genomic sequences<sup>10</sup> to detect patterns or risk factors that may be overlooked by traditional methods. In well-designed pipelines, AI can assist with tasks ranging from automated data cleaning to advance predictive modeling, thereby improving efficiency and standardization. Large language models (LLMs)-such as ChatGPT-and automated ML (AutoML) systems can already perform routine statistical tasks, such as conducting standard tests and proposing or tuning candidate models, effectively acting as statistical assistants.<sup>11</sup> When combined with AutoML and code‑execution environments, LLMs can even transform a dataset and a research question into a first‑draft analysis report. Nonetheless, experts must still verify model specifications, check assumptions, control for multiplicity, and ensure reproducibility.<sup>12</sup></p>\r\n<p>Despite these advances, AI-driven automation has limitations. While AI models often excel at interpolating within known datasets, they may fail on out-of-sample data or when biases are present.<sup>13</sup> Key concerns-such as lack of transparency (&ldquo;black box&rdquo; models), overfitting, and dependence on high‑quality training data-highlight the need for human oversight. As automation expands across the analytical pipeline, integrity, transparency, and interpretability still require multidisciplinary input. Statisticians, clinicians, ethicists, and regulators must work together to ensure rigorous study design, model validation, bias assessment, and reproducible reporting.<sup>14</sup> In practice, multidisciplinary collaboration is critical. Clinicians should partner with statisticians from the outset of study design through analysis. Ko&ccedil;ak et al.<sup>15</sup> recently warned that a &ldquo;language barrier&rdquo; between AI developers and clinicians can result in errors or low-quality studies. Engaging clinical experts early helps ensure that AI models address medically relevant questions, use appropriate endpoints, and are evaluated using clinically meaningful-not merely technical-metrics.</p>\r\n<p>Emphasize transparency and reproducibility: Preregister studies when possible, share analysis code and data (with appropriate patient privacy safeguards), and document each step of the process. This enables others to verify findings and reuse analytical pipelines.</p>\r\n<p>Move beyond<em> p -</em>values as sole evidence: Report effect sizes, confidence intervals, predictive accuracy metrics, and where feasible, Bayesian measures. Interpret findings in light of clinical relevance and existing knowledge-not just statistical thresholds.</p>\r\n<p>Validate AI models rigorously: Use true hold-out datasets or external cohorts to assess generalizability. Prevent data leakage and overfitting through careful separation of training, validation, and test sets. Always report model performance on independent data, when available.</p>\r\n<p>Encourage interdisciplinary expertise: Include clinicians, statisticians, and computer scientists as part of research teams. Promote cross-training: clinicians should acquire basic AI/ML literacy, while statisticians and data scientists should understand the biomedical context. Bridging these gaps reduces the risk of methodological errors.</p>\r\n<p>Practice continuous education: Stay current with evolving guidelines for AI in medicine (e.g., CONSORT-AI, TRIPOD-AI, CLAIM), and monitor updates from statistical and regulatory authorities. Journal editors and peer reviewers should also improve their AI/statistics literacy to ensure critical and informed manuscript evaluation.</p>\r\n<p>These measures will help ensure that AI enhances-rather than undermines-statistical rigor in biomedical research.</p>\r\n<p>AI is transforming medical research by enabling analyses that were previously impractical<sup>16</sup>, but this shift brings greater responsibility to uphold scientific rigor. Clinicians and researchers should embrace these new tools while remaining cautious of overly simplistic or unverified findings. Robust statistical thinking-emphasizing transparency, thorough validation, and interdisciplinary collaboration-remains as essential as ever. Ultimately, the goal is not to replace hypothesis-driven medicine but to enhance it. This requires using AI-driven insights that are trustworthy, reproducible, and truly beneficial to patients. By balancing innovation with rigor, the medical research community can ensure that the promise of AI is fulfilled in a reliable and clinically meaningful way.</p>\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t<!--\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t<div\tstyle=\"width:20%; float:left; min-height:150px; padding:10px; padding-top:40px;\">\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t\t\t\t\t<a class=\"selected_menu\" href=\"#introduction\">Introduction</a><a class=\"menu\" href=\"#reference\">Reference</a>\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t-->\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t<div  style=\"width:100%; float:left;  padding-bottom:0px;\">\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t<div\tstyle=\"width:100%; float:left; min-height:45px; text-align:justify\">\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t\t<a href=\"\" name=\"reference\"></a>\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t\t<!--<h2>Reference</h2>-->\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t\t<h3>References</h3>\r\n<ol>\r\n<li>Han H. Challenges of reproducible AI in biomedical data science. <em>BMC Med Genomics. </em>2025;18:8.</li>\r\n<li>Ball P. Is AI leading to a reproducibility crisis in science? <em>Nature. </em>2023;624:22-25.</li>\r\n<li>Greenland S, Senn SJ, Rothman KJ, et al. Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations. <em>Eur J Epidemiol. </em>2016;31:337-350.</li>\r\n<li>Ch&eacute;n OY, Bodelet JS, Saraiva RG, et al. The roles, challenges, and merits of the p value. <em>Patterns (N Y)</em>. 2023;4:100878.</li>\r\n<li>Wasserstein RL, Lazar NA. The ASA statement on p-values: context, process, and purpose. <em>The American Statistician. </em>2016;70:129-133.</li>\r\n<li>Perneger TV. How to use likelihood ratios to interpret evidence from randomized trials. <em>J Clin Epidemiol</em>. 2021;136:235-242.</li>\r\n<li>Goodman SN. Toward evidence-based medical statistics. 2: the Bayes factor. <em>Ann Intern Med</em>. 1999;130:1005-1013.</li>\r\n<li>Usta U, Taştekin E. Present and future of artificial intelligence in pathology. <em>Balkan Med J. </em>2024;41:157-158.</li>\r\n<li>Sarıkaya Solak S, G&ouml;ktay F. The promising role of artificial intelligence in nail diseases. <em>Balkan Med J. </em>2024;41:234-235.</li>\r\n<li>Erdoğan S. Integration of Artificial intelligence and genome editing system for determining the treatment of genetic disorders. <em>Balkan Med J. </em>2024;41:419-420.</li>\r\n<li>Ko&ccedil;ak D. Examination of ChatGPT&rsquo;s performance as a data analysis tool. <em>Educ Psychol Meas. </em>2025:00131644241302721.</li>\r\n<li>Korkmaz S. Artificial intelligence in healthcare: a revolutionary ally or an ethical dilemma? <em>Balkan Med J. </em>2024;41:87-88.</li>\r\n<li>Mondillo G, Frattolillo V, Colosimo S, Perrotta A. Artificial intelligence in pediatric nail diseases: limitations and prospects. <em>Balkan Med J. </em>2025;42:86.</li>\r\n<li>Collins GS, Moons KGM, Dhiman P, et al. TRIPOD+AI statement: updated guidance for reporting clinical prediction models that use regression or machine learning methods<em>. BMJ. </em>2024;385:e078378. Erratum in:<em> BMJ</em>. 2024;385:q902.</li>\r\n<li>Ko&ccedil;ak B, Cuocolo R, dos Santos DP, Stanzione A, Ugga L. Must-have Qualities of clinical research on artificial intelligence and machine learning<em>. Balkan Med J. </em>2023;40:3-12.</li>\r\n<li>Hayıroğlu Mİ, Altay S. The role of artificial intelligence in coronary artery disease and atrial fibrillation. <em>Balkan Med J. </em>2023;40:151-152.</li>\r\n</ol>\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t<!--\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t<div\tstyle=\"width:20%; float:left; min-height:150px; padding:10px; padding-top:40px;\">\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t\t\t\t\t<a class=\"menu\" href=\"#introduction\">Introduction</a><a class=\"selected_menu\" href=\"#reference\">Reference</a>\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t-->\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t        \t\t\t\n\t\t\n\t\t\t\t\t\t\t\t\t\t\t        \t</div>\n\t\t\t<";
